{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001d1afc246a7964130f43ae940af6bc6c57f01</td>\n",
       "      <td>By . Associated Press . PUBLISHED: . 14:11 EST...</td>\n",
       "      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002095e55fcbd3a2f366d9bf92a95433dc305ef</td>\n",
       "      <td>(CNN) -- Ralph Mata was an internal affairs li...</td>\n",
       "      <td>Criminal complaint: Cop used his role to help ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00027e965c8264c35cc1bc55556db388da82b07f</td>\n",
       "      <td>A drunk driver who killed a young woman in a h...</td>\n",
       "      <td>Craig Eccleston-Todd, 27, had drunk at least t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002c17436637c4fe1837c935c04de47adb18e9a</td>\n",
       "      <td>(CNN) -- With a breezy sweep of his pen Presid...</td>\n",
       "      <td>Nina dos Santos says Europe must be ready to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0003ad6ef0c37534f80b55b4235108024b407f0b</td>\n",
       "      <td>Fleetwood are the only team still to have a 10...</td>\n",
       "      <td>Fleetwood top of League One after 2-0 win at S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id   \n",
       "0  0001d1afc246a7964130f43ae940af6bc6c57f01  \\\n",
       "1  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n",
       "2  00027e965c8264c35cc1bc55556db388da82b07f   \n",
       "3  0002c17436637c4fe1837c935c04de47adb18e9a   \n",
       "4  0003ad6ef0c37534f80b55b4235108024b407f0b   \n",
       "\n",
       "                                             article   \n",
       "0  By . Associated Press . PUBLISHED: . 14:11 EST...  \\\n",
       "1  (CNN) -- Ralph Mata was an internal affairs li...   \n",
       "2  A drunk driver who killed a young woman in a h...   \n",
       "3  (CNN) -- With a breezy sweep of his pen Presid...   \n",
       "4  Fleetwood are the only team still to have a 10...   \n",
       "\n",
       "                                          highlights  \n",
       "0  Bishop John Folda, of North Dakota, is taking ...  \n",
       "1  Criminal complaint: Cop used his role to help ...  \n",
       "2  Craig Eccleston-Todd, 27, had drunk at least t...  \n",
       "3  Nina dos Santos says Europe must be ready to a...  \n",
       "4  Fleetwood top of League One after 2-0 win at S...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"../cnn_dailymail/train.csv\")\n",
    "val_data = pd.read_csv(\"../cnn_dailymail/validation.csv\")\n",
    "test_data = pd.read_csv(\"../cnn_dailymail/test.csv\")\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(['id'], axis=1)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.drop(['id'], axis=1)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/guilherme/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clean_text(text, remove_stopwords=True):\n",
    "    # Lowercase all letters\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    tmp = []\n",
    "    # Transform contractions\n",
    "    for word in text:\n",
    "        if word in contractions:\n",
    "            tmp.append(contractions[word])\n",
    "        else:\n",
    "            tmp.append(word)\n",
    "    text = ' '.join(tmp)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    # Remove <a HTML character\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    # Remove &amp character\n",
    "    text = re.sub(r'&amp;', '', text)\n",
    "    # Remove some special characters\n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    # Remove the <br /> HTML character\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    # Remove single quotation marks\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words('english'))\n",
    "        text = [w for w in text if w not in stops]\n",
    "        text = ' '.join(text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Summaries Complete\n",
      "Cleaning Texts Complete\n"
     ]
    }
   ],
   "source": [
    "# Get list of cleaned summaries (not removing stopwords)\n",
    "clean_summaries = []\n",
    "for summary in train_data.highlights:\n",
    "    clean_summaries.append(clean_text(summary, remove_stopwords=False))\n",
    "print('Cleaning Summaries Complete')\n",
    "\n",
    "# Get list of cleaned training texts (removing stopwords)\n",
    "clean_texts = []\n",
    "for text in train_data.article:\n",
    "    clean_texts.append(clean_text(text))\n",
    "print('Cleaning Texts Complete')\n",
    "\n",
    "# Delete the training data variable to save space\n",
    "del train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe to store the training data to be used (not all data is used to minimize memory consumption)\n",
    "clean_df = pd.DataFrame()\n",
    "clean_df['text'] = clean_texts[:10000]\n",
    "clean_df['summary'] = clean_summaries[:10000]\n",
    "# Drop rows with empty summaries\n",
    "clean_df['summary'].replace('', np.nan, inplace=True)\n",
    "clean_df.dropna(axis=0, inplace=True)\n",
    "\n",
    "# Add start of sentence and end of sentence tokens to the summaries (targets)\n",
    "clean_df['summary'] = clean_df['summary'].apply(lambda x: '<sostok>' + ' ' + x + ' ' + '<eostok>')\n",
    "\n",
    "# Delete variables that will not be used anymore\n",
    "del clean_texts\n",
    "del clean_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "train_x, test_x, train_y, test_y = train_test_split(clean_df['text'], clean_df['summary'], test_size=0.1, random_state=0)\n",
    "del clean_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Tokenize your text and count the words\n",
    "word_counter = Counter()\n",
    "for sentence in train_x:\n",
    "    word_counter.update(tokenizer(sentence))\n",
    "\n",
    "# Then you can continue as before\n",
    "thresh = 4\n",
    "count = 0\n",
    "total_count = 0\n",
    "frequency = 0\n",
    "total_frequency = 0\n",
    "\n",
    "for key, value in word_counter.items():\n",
    "    total_count += 1\n",
    "    total_frequency += value\n",
    "    if value < thresh:\n",
    "        count += 1\n",
    "        frequency += value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary:  58.125684692759684\n",
      "Total Coverage of rare words:  2.448459711489284\n",
      "Text Vocab:  42046\n"
     ]
    }
   ],
   "source": [
    "print('% of rare words in vocabulary: ', (count/total_count)*100.0)\n",
    "print('Total Coverage of rare words: ', (frequency/total_frequency)*100.0)\n",
    "t_max_features = total_count - count\n",
    "print('Text Vocab: ', t_max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but for summaries and have a higher threshold (word needs to appear a minimum of 6 times to not be rare - rare words are not included in the vocab)\n",
    "\n",
    "# Get a tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Tokenize your text and count the words\n",
    "word_counter = Counter()\n",
    "for sentence in train_y:\n",
    "    word_counter.update(tokenizer(sentence))\n",
    "\n",
    "# Then you can continue as before\n",
    "thresh = 6\n",
    "count = 0\n",
    "total_count = 0\n",
    "frequency = 0\n",
    "total_frequency = 0\n",
    "\n",
    "for key, value in word_counter.items():\n",
    "    total_count += 1\n",
    "    total_frequency += value\n",
    "    if value < thresh:\n",
    "        count += 1\n",
    "        frequency += value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary:  77.1330121363664\n",
      "Total Coverage of rare words:  9.840701372995234\n",
      "Summary Vocab:  7499\n"
     ]
    }
   ],
   "source": [
    "print('% of rare words in vocabulary: ', (count/total_count)*100.0)\n",
    "print('Total Coverage of rare words: ', (frequency/total_frequency)*100.0)\n",
    "s_max_features = total_count-count\n",
    "print('Summary Vocab: ', s_max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a max length for both the article and the summary - if any goes over this limit, only the first part of the sample will be included (first n tokens)\n",
    "maxlen_text = 800\n",
    "maxlen_summ = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x = test_x\n",
    "# Convert text data to integer IDs\n",
    "word_counter_x = Counter()\n",
    "for sentence in train_x:\n",
    "    word_counter_x.update(sentence)\n",
    "\n",
    "# Keep only the most common `t_max_features` tokens\n",
    "most_common_words_x = word_counter_x.most_common(t_max_features)\n",
    "vocab_x = {word: i+1 for i, (word, _) in enumerate(most_common_words_x)}  # +1 to leave 0 for padding\n",
    "\n",
    "# Convert tokens to numerical ids using the built vocab_x\n",
    "train_x = [[vocab_x.get(token, 0) for token in tokens] for tokens in train_x]  # 0 for unknown tokens\n",
    "val_x = [[vocab_x.get(token, 0) for token in tokens] for tokens in val_x]  # 0 for unknown tokens\n",
    "\n",
    "# Pad the sequences\n",
    "train_x = pad_sequence([torch.tensor(sentence) for sentence in train_x], batch_first=True, padding_value=0)\n",
    "val_x = pad_sequence([torch.tensor(sentence) for sentence in val_x], batch_first=True, padding_value=0)\n",
    "\n",
    "# Truncate or pad each sequence to have exactly `maxlen_text` tokens\n",
    "if train_x.size(1) > maxlen_text:\n",
    "    train_x = train_x[:, :maxlen_text]\n",
    "else:\n",
    "    train_x = F.pad(train_x, (0, maxlen_text - train_x.size(1)), value=0)\n",
    "\n",
    "if val_x.size(1) > maxlen_text:\n",
    "    val_x = val_x[:, :maxlen_text]\n",
    "else:\n",
    "    val_x = F.pad(val_x, (0, maxlen_text - val_x.size(1)), value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_y = test_y\n",
    "# Convert text data to integer IDs\n",
    "word_counter_y = Counter()\n",
    "for sentence in train_y:\n",
    "    word_counter_y.update(sentence)\n",
    "\n",
    "# Keep only the most common `t_max_features` tokens\n",
    "most_common_words_y = word_counter_y.most_common(s_max_features)\n",
    "vocab_y = {word: i+1 for i, (word, _) in enumerate(most_common_words_y)}  # +1 to leave 0 for padding\n",
    "\n",
    "# Convert tokens to numerical ids using the built vocab_y\n",
    "train_y = [[vocab_y.get(token, 0) for token in tokens] for tokens in train_y]  # 0 for unknown tokens\n",
    "val_y = [[vocab_y.get(token, 0) for token in tokens] for tokens in val_y]  # 0 for unknown tokens\n",
    "\n",
    "# Pad the sequences\n",
    "train_y = pad_sequence([torch.tensor(sentence) for sentence in train_y], batch_first=True, padding_value=0)\n",
    "val_y = pad_sequence([torch.tensor(sentence) for sentence in val_y], batch_first=True, padding_value=0)\n",
    "\n",
    "# Truncate or pad each sequence to have exactly `maxlen_summ` tokens\n",
    "if train_y.size(1) > maxlen_summ:\n",
    "    train_y = train_y[:, :maxlen_summ]\n",
    "else:\n",
    "    train_y = F.pad(train_y, (0, maxlen_summ - train_y.size(1)), value=0)\n",
    "\n",
    "if val_y.size(1) > maxlen_summ:\n",
    "    val_y = val_y[:, :maxlen_summ]\n",
    "else:\n",
    "    val_y = F.pad(val_y, (0, maxlen_summ - val_y.size(1)), value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Sequence torch.Size([9000, 800])\n",
      "Target Values Shape torch.Size([9000, 150])\n",
      "Test Sequence torch.Size([1000, 800])\n",
      "Target Test Shape torch.Size([1000, 150])\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Sequence\", train_x.shape)\n",
    "print('Target Values Shape', train_y.shape)\n",
    "print('Test Sequence', val_x.shape)\n",
    "print('Target Test Shape', val_y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_index = {}\n",
    "embed_dim = 100\n",
    "with open('./glove.6B.100d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embed_index[word] = coefs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_embed = torch.zeros((t_max_features, embed_dim))\n",
    "for word, i in vocab_x.items():\n",
    "    vec = embed_index.get(word)\n",
    "    if i < t_max_features and vec is not None:\n",
    "        t_embed[i] = torch.from_numpy(vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_embed = torch.zeros((s_max_features, embed_dim))\n",
    "for word, i in vocab_y.items():\n",
    "    vec = embed_index.get(word)\n",
    "    if i < s_max_features and vec is not None:\n",
    "        s_embed[i] = torch.from_numpy(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del embed_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, enc_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "\n",
    "        # Concatenate the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, dec_hid_dim)\n",
    "        self.fc_out = nn.Linear(dec_hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, hidden.unsqueeze(0))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "\n",
    "        return prediction, hidden.squeeze(0)\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        input = trg[0,:]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            outputs[t] = output\n",
    "            top1 = output.argmax(1) \n",
    "\n",
    "            input = trg[t] if random.random() < teacher_forcing_ratio else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PAD_IDX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mimport\u001b[39;00m cross_entropy\n\u001b[1;32m      5\u001b[0m \u001b[39m# Define loss function (criterion) and optimizer\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[39m=\u001b[39mPAD_IDX)\n\u001b[1;32m      7\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters())\n\u001b[1;32m      9\u001b[0m \u001b[39m# Define the early stopping criteria\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PAD_IDX' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "# Define loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Define the early stopping criteria\n",
    "no_improve = 0\n",
    "min_val_loss = float('inf')\n",
    "early_stop_epochs = 5  # stop if no improvement for 5 epochs\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        input_tensor = batch[0].to(device)\n",
    "        target_tensor = batch[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_tensor, target_tensor)\n",
    "        \n",
    "        # Pack the sequence of the output tensor to calculate loss\n",
    "        output_packed = pack_padded_sequence(output, lengths, batch_first=True).data\n",
    "        target_packed = pack_padded_sequence(target_tensor, lengths, batch_first=True).data\n",
    "        \n",
    "        loss = criterion(output_packed, target_packed)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch in val_dataloader:\n",
    "            input_tensor = batch[0].to(device)\n",
    "            target_tensor = batch[1].to(device)\n",
    "            \n",
    "            output = model(input_tensor, target_tensor)\n",
    "            \n",
    "            # Pack the sequence of the output tensor to calculate loss\n",
    "            output_packed = pack_padded_sequence(output, lengths, batch_first=True).data\n",
    "            target_packed = pack_padded_sequence(target_tensor, lengths, batch_first=True).data\n",
    "            \n",
    "            loss = criterion(output_packed, target_packed)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch}, Train Loss: {epoch_loss / len(train_dataloader)}, Val Loss: {val_loss / len(val_dataloader)}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve == early_stop_epochs:\n",
    "            print('Early stopping')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq_attention",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
