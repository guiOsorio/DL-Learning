{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from translation.utils import translate_sentence, bleu, save_checkpoint, load_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de-core-news-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.5.0/de_core_news_sm-3.5.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from de-core-news-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.49.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.29.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.49.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.29.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/guilherme/anaconda3/envs/seq_attention/lib/python3.11/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download de_core_news_sm\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "spacy_ger = spacy.load(\"de_core_news_sm\")\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ger(text):\n",
    "    return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
    "\n",
    "def tokenize_eng(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "german = Field(tokenize=tokenize_ger, lower=True, init_token='<sos>', eos_token='<eos>')\n",
    "english = Field(tokenize=tokenize_eng, lower=True, init_token='<sos>', eos_token='<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(german, english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "german.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "english.build_vocab(train_data, max_size=10000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length, N) N being the batch size\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (seq_length, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "\n",
    "        return hidden, cell\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # shape of x: (N) but we want (1, N)\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        # shape of outputs: (1, N, hidden_size)\n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "        # shape of prediction: (1, N, length_of_vocab)\n",
    "\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        return predictions, hidden, cell\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "         batch_size = source.shape[1]\n",
    "         target_len = target.shape[0]\n",
    "         target_vocab_size = len(english.vocab)\n",
    "\n",
    "         outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "         hidden, cell = self.encoder(source)\n",
    "\n",
    "        # Grab start token\n",
    "         x = target[0]\n",
    "\n",
    "         for t in range(1, target_len):\n",
    "             output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "\n",
    "             outputs[t] = output\n",
    "\n",
    "             best_guess = output.argmax(1)\n",
    "\n",
    "             x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "        \n",
    "         return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['true', 'sacks', 'hopscotch', 'hopscotch', 'crammed', 'saving', 'saving', 'shaped', 'true', 'pineapple', 'true', 'pads', 'garb', 'place', 'glassy', 'glassy', 'like', 'true', 'like', 'skimpy', 'true', 'skimpy', 'wineglasses', 'embraces', 'balance', 'true', 'desks', 'pads', 'mets', 'mets', 'embraces', 'embraces', 'true', 'offers', 'balance', 'desks', 'pads', 'mets', 'perched', 'mets', 'sip', 'pounces', 'midair', 'batman', 'batman', 'cake', 'archway', 'graffitied', 'needed', 'cabin']\n",
      "[Epoch 1 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'young', 'in', 'a', 'blue', 'shirt', 'and', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "[Epoch 2 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', '<unk>', 'with', 'a', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', '<eos>']\n",
      "[Epoch 3 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', '<unk>', 'with', 'with', '<unk>', '<unk>', '<unk>', '<unk>', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "[Epoch 4 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', '<unk>', 'with', 'with', 'two', '<unk>', '<unk>', 'a', '<unk>', 'of', 'a', 'a', 'a', '.', '<eos>']\n",
      "[Epoch 5 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'man', 'with', 'with', '<unk>', 'of', 'a', 'to', 'a', 'a', 'a', 'large', 'large', '.', '.', '<eos>']\n",
      "[Epoch 6 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'man', 'with', 'many', 'men', 'are', 'to', 'to', 'a', 'a', 'a', 'a', 'large', 'large', '.', '.', '<eos>']\n",
      "[Epoch 7 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'police', 'officer', 'with', 'two', 'men', 'men', 'are', 'being', 'pulled', 'by', 'a', 'large', 'large', '.', '.', '<eos>']\n",
      "[Epoch 8 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'men', 'with', 'men', 'are', 'being', 'pulled', 'from', 'a', 'large', 'large', 'large', 'large', 'large', '.', '.', '<eos>']\n",
      "[Epoch 9 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'many', 'men', 'pulled', 'by', 'a', 'a', 'a', 'a', 'large', '<unk>', '.', '<eos>']\n",
      "[Epoch 10 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'men', 'with', 'men', 'men', 'are', 'pulled', 'by', 'a', 'large', 'of', 'a', 'large', 'large', 'of', '.', '<eos>']\n",
      "[Epoch 11 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'many', 'men', 'pulled', 'by', 'a', 'large', 'by', 'a', 'large', 'large', '.', '<eos>']\n",
      "[Epoch 12 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'many', 'men', 'pulled', 'pulled', 'by', 'a', 'by', 'a', 'by', 'a', 'large', '.', '.', '<eos>']\n",
      "[Epoch 13 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'many', 'men', 'pulled', 'by', 'a', 'large', 'by', 'a', 'large', 'large', 'of', 'fish', '.', '<eos>']\n",
      "[Epoch 14 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'many', 'men', 'pulled', 'pulled', 'by', 'a', 'large', ',', 'by', 'a', 'large', 'large', 'bull', '.', '<eos>']\n",
      "[Epoch 15 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'men', 'men', 'pulled', 'pulled', 'by', 'a', 'large', 'by', 'a', 'large', 'fish', '.', '<eos>']\n",
      "[Epoch 16 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'pulled', 'by', 'a', 'by', 'a', 'large', 'fish', '.', '<eos>']\n",
      "[Epoch 17 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'many', 'men', 'pulled', 'pulled', 'by', 'a', 'large', ',', 'by', 'a', 'large', 'fish', '.', '<eos>']\n",
      "[Epoch 18 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'by', 'a', 'large', 'by', 'a', 'large', 'flags', '.', '<eos>']\n",
      "[Epoch 19 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'by', 'a', 'large', 'by', 'a', 'large', 'bull', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "# Model hyperparameters\n",
    "load_model = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size_encoder = len(german.vocab)\n",
    "input_size_decoder = len(english.vocab)\n",
    "output_size = len(english.vocab)\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "# Tensorboard\n",
    "writer = SummaryWriter(f'runs/loss_plot')\n",
    "step = 0\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, val_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch = True,\n",
    "    sort_key = lambda x: len(x.src),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout).to(device)\n",
    "decoder_net = Decoder(input_size_decoder, decoder_embedding_size, hidden_size, output_size, num_layers, dec_dropout).to(device)\n",
    "\n",
    "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "pad_idx = english.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "if load_model:\n",
    "    load_checkpoint(torch.load('my_checkpoint.pth.ptar'), model, optimizer)\n",
    "\n",
    "sentence = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen.\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "\n",
    "    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "    save_checkpoint(checkpoint)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    translated_sentence = translate_sentence(\n",
    "        model, sentence, german, english, device, max_length=50\n",
    "    )\n",
    "\n",
    "    print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        inp_data = batch.src.to(device)\n",
    "        target = batch.trg.to(device)\n",
    "\n",
    "        output = model(inp_data, target)\n",
    "        # output shape: (trg_len, batch_size, output_dim)\n",
    "\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "\n",
    "        writer.add_scalar('Training loss', loss, global_step=step)\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'germanEnglish.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq_attention",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
