{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from utils import translate_sentence, bleu, save_checkpoint, load_checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001d1afc246a7964130f43ae940af6bc6c57f01</td>\n",
       "      <td>By . Associated Press . PUBLISHED: . 14:11 EST...</td>\n",
       "      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002095e55fcbd3a2f366d9bf92a95433dc305ef</td>\n",
       "      <td>(CNN) -- Ralph Mata was an internal affairs li...</td>\n",
       "      <td>Criminal complaint: Cop used his role to help ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00027e965c8264c35cc1bc55556db388da82b07f</td>\n",
       "      <td>A drunk driver who killed a young woman in a h...</td>\n",
       "      <td>Craig Eccleston-Todd, 27, had drunk at least t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002c17436637c4fe1837c935c04de47adb18e9a</td>\n",
       "      <td>(CNN) -- With a breezy sweep of his pen Presid...</td>\n",
       "      <td>Nina dos Santos says Europe must be ready to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0003ad6ef0c37534f80b55b4235108024b407f0b</td>\n",
       "      <td>Fleetwood are the only team still to have a 10...</td>\n",
       "      <td>Fleetwood top of League One after 2-0 win at S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  \\\n",
       "0  0001d1afc246a7964130f43ae940af6bc6c57f01   \n",
       "1  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n",
       "2  00027e965c8264c35cc1bc55556db388da82b07f   \n",
       "3  0002c17436637c4fe1837c935c04de47adb18e9a   \n",
       "4  0003ad6ef0c37534f80b55b4235108024b407f0b   \n",
       "\n",
       "                                             article  \\\n",
       "0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
       "1  (CNN) -- Ralph Mata was an internal affairs li...   \n",
       "2  A drunk driver who killed a young woman in a h...   \n",
       "3  (CNN) -- With a breezy sweep of his pen Presid...   \n",
       "4  Fleetwood are the only team still to have a 10...   \n",
       "\n",
       "                                          highlights  \n",
       "0  Bishop John Folda, of North Dakota, is taking ...  \n",
       "1  Criminal complaint: Cop used his role to help ...  \n",
       "2  Craig Eccleston-Todd, 27, had drunk at least t...  \n",
       "3  Nina dos Santos says Europe must be ready to a...  \n",
       "4  Fleetwood top of League One after 2-0 win at S...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# val_df = pd.read_csv(\"./cnn_dailymail/validation.csv\")\n",
    "# test_df = pd.read_csv(\"./cnn_dailymail/test.csv\")\n",
    "train_df = pd.read_csv(\"./cnn_dailymail/train.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>By . Associated Press . PUBLISHED: . 14:11 EST...</td>\n",
       "      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(CNN) -- Ralph Mata was an internal affairs li...</td>\n",
       "      <td>Criminal complaint: Cop used his role to help ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A drunk driver who killed a young woman in a h...</td>\n",
       "      <td>Craig Eccleston-Todd, 27, had drunk at least t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(CNN) -- With a breezy sweep of his pen Presid...</td>\n",
       "      <td>Nina dos Santos says Europe must be ready to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fleetwood are the only team still to have a 10...</td>\n",
       "      <td>Fleetwood top of League One after 2-0 win at S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
       "1  (CNN) -- Ralph Mata was an internal affairs li...   \n",
       "2  A drunk driver who killed a young woman in a h...   \n",
       "3  (CNN) -- With a breezy sweep of his pen Presid...   \n",
       "4  Fleetwood are the only team still to have a 10...   \n",
       "\n",
       "                                          highlights  \n",
       "0  Bishop John Folda, of North Dakota, is taking ...  \n",
       "1  Criminal complaint: Cop used his role to help ...  \n",
       "2  Craig Eccleston-Todd, 27, had drunk at least t...  \n",
       "3  Nina dos Santos says Europe must be ready to a...  \n",
       "4  Fleetwood top of League One after 2-0 win at S...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.drop(columns=['id'])\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_to_tokenize = []\n",
    "summaries_to_tokenize = []\n",
    "data = train_df.sample(1100)\n",
    "articles = list(data['article'])\n",
    "summaries = list(data['highlights'])\n",
    "for i in range(len(data)):\n",
    "    article = data.iloc[i]['article']\n",
    "    summary = data.iloc[i]['highlights']\n",
    "    new_summary = \"<sos> \" + summary + \" <eos>\"\n",
    "    articles_to_tokenize.append(article)\n",
    "    summaries_to_tokenize.append(new_summary)\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_article_tokens():\n",
    "    for article in articles_to_tokenize:\n",
    "        tokens = tokenizer(article)\n",
    "        yield tokens\n",
    "\n",
    "articles_token_generator = yield_article_tokens()\n",
    "articles_vocab = build_vocab_from_iterator(articles_token_generator, min_freq=5, specials=['<unk>'], max_tokens=20000) # to be used in the encoder\n",
    "articles_vocab.set_default_index(articles_vocab['<unk>'])\n",
    "\n",
    "def yield_summary_tokens():\n",
    "    for summary in summaries_to_tokenize:\n",
    "        tokens = tokenizer(summary)\n",
    "        yield tokens\n",
    "\n",
    "summaries_token_generator = yield_summary_tokens()\n",
    "summaries_vocab = build_vocab_from_iterator(summaries_token_generator, min_freq=3, specials=['<sos>', '<unk>', '<eos>'], max_tokens=20000) # to be used in the decoder\n",
    "summaries_vocab.set_default_index(summaries_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to tokenize a summary and get back its original version\n",
    "\n",
    "summary_to_tokenize = \"<sos> \" + data.iloc[0]['highlights'] + \" <eos>\"\n",
    "tokenized_summary = tokenizer(summary_to_tokenize)\n",
    "\n",
    "tokens = [summaries_vocab[token] for token in tokenized_summary]\n",
    "\n",
    "def tokens_to_text(tokens, vocab):\n",
    "    itos = vocab.get_itos()  # Get the list of tokens\n",
    "    return \" \".join(itos[index] for index in tokens)\n",
    "\n",
    "original_summary = tokens_to_text(tokens, summaries_vocab)\n",
    "# data.iloc[0]['highlights'], original_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11869, 3114)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_vocab_size = len(articles_vocab)\n",
    "decoder_vocab_size = len(summaries_vocab)\n",
    "encoder_vocab_size, decoder_vocab_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARCHITECTURE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, n_layers, dropout_p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size) # (vocab size of articles - encoder input, embedding dimensions)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, n_layers, bidirectional=True)\n",
    "\n",
    "        self.fc_hidden = nn.Linear(hidden_size*2, hidden_size) # hidden state\n",
    "        self.fc_cell = nn.Linear(hidden_size*2, hidden_size) # LSTM cell output\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Create embedding based on current token x\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "\n",
    "        # Pass embedding to the LSTM unit\n",
    "        # concatenated hidden states (since LSTM is bidirectional) for each token in the sequence, hidden states (one for each direction) of last layer, cell states of last layer\n",
    "        encoder_states, (hidden, cell) = self.rnn(embedding)\n",
    "\n",
    "        # Concatenate last hidden state returned by the LSTM on dimension 2 (2,1,50) -> (1,1,100), then pass that to a linear layer to reduce it to (1,1,50) for the decoder\n",
    "        hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n",
    "        cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n",
    "\n",
    "        return encoder_states, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, n_layers, dropout_p):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(hidden_size*2 + embedding_size, hidden_size, n_layers) # will take the context vector and the embedding of the current token\n",
    "\n",
    "        self.energy = nn.Linear(hidden_size*3, 1) \n",
    "        # will calculate the energy for each encoder token for the current step based on its encoder states (hidden_size*2 because of bidirectional LSTM) and the last hidden state\n",
    "        self.fc = nn.Linear(hidden_size, output_size) # fianl fully-connected layer\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, encoder_states, hidden, cell):\n",
    "        # Add a dimension to x: 1 --> 2. x here is the current token\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        # Create embedding for the current token\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "\n",
    "        # Calculate energy for the attention mechanism\n",
    "        sequence_length = encoder_states.shape[0] # number of tokens in the sequence\n",
    "        h_reshaped = hidden.repeat(sequence_length, 1, 1) # make a copy of the last hidden state of the encoder for every token in the sequence\n",
    "        energy = self.relu(self.energy(torch.cat((h_reshaped, encoder_states), dim=2)))\n",
    "\n",
    "        # Pass energy score through a softmax to get the attention score for each token in the sequence\n",
    "        attention = self.softmax(energy)\n",
    "\n",
    "        # Compute the context vector for the current token based on the attention scores and encoder states\n",
    "        # attention: (seq_length, N, 1), snk\n",
    "        # encoder_states: (seq_length, N, hidden_size*2), snl\n",
    "        # we want context_vector: (1, N, hidden_size*2), i.e knl\n",
    "        context_vector = torch.einsum(\"snk,snl->knl\", attention, encoder_states)\n",
    "\n",
    "        # Pass the concatenated context vector and embedding through an LSTM layer. The hidden and cell states will be used in the next pass through the decoder\n",
    "        rnn_input = torch.cat((context_vector, embedding), dim=2)\n",
    "        outputs, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "\n",
    "        # Make the prediction based on the outputs of the LSTM layer, passing it into a final linear layer\n",
    "        predictions = self.fc(outputs).squeeze(0)\n",
    "        \n",
    "        return predictions, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3114])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test pass on a sample sentence\n",
    "source = data.iloc[0]['article']\n",
    "source_ = \"<sos> \" + source + \" <eos>\"\n",
    "tokenized_source = tokenizer(source_)\n",
    "tokens = [articles_vocab[token] for token in tokenized_source]\n",
    "article_tensor = torch.LongTensor(tokens).unsqueeze(1)\n",
    "\n",
    "target = data.iloc[0]['highlights']\n",
    "target_ = \"<sos> \" + target + \" <eos>\"\n",
    "tokenized_target = tokenizer(target_)\n",
    "tgt_tokens = [summaries_vocab[token] for token in tokenized_target]\n",
    "summary_tensor = torch.LongTensor(tgt_tokens).unsqueeze(1)\n",
    "\n",
    "embedding_size = 100\n",
    "hidden_size = 50\n",
    "n_layers = 1\n",
    "dropout_p = 0.2\n",
    "\n",
    "enc = Encoder(encoder_vocab_size, embedding_size, hidden_size, n_layers, dropout_p)\n",
    "encoder_states, hidden, cell = enc(article_tensor)\n",
    "encoder_states.shape, hidden.shape, cell.shape\n",
    "\n",
    "output_size = decoder_vocab_size\n",
    "current_word = \"hello\"\n",
    "tokenized_word = tokenizer(current_word)\n",
    "current_token = [summaries_vocab[token] for token in tokenized_word]\n",
    "current_tensor = torch.LongTensor(current_token)\n",
    "\n",
    "dec = Decoder(decoder_vocab_size, embedding_size, hidden_size, output_size, n_layers, dropout_p)\n",
    "predictions, hidden, cell = dec(current_tensor, encoder_states, hidden, cell)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        batch_size = source.shape[1] # number of sequences in the source batch\n",
    "        target_len = target.shape[0] # number of tokens in the target vector\n",
    "        target_vocab_size = decoder_vocab_size\n",
    "\n",
    "        # outputs variable to store the predictions - a prediction for each target token in the batch, the prediction is a one-hot encoded vector\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "        # run encoder through the batch's sequences\n",
    "        encoder_states, hidden, cell = self.encoder(source)\n",
    "\n",
    "        # Define the first decoder input token - will always be the <sos> token\n",
    "        x = target[0]\n",
    "\n",
    "        # Loop through every token in the target after the <sos>\n",
    "        for t in range(1, target_len):\n",
    "            # run decoder based on encoder's outputs and update the hidden and cell states\n",
    "            output, hidden, cell = self.decoder(x, encoder_states, hidden, cell)\n",
    "\n",
    "            # Store prediction for time step t (current time step)\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Best word prediction - index in the vocab\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            # Teacher forcing\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "\n",
    "        return outputs # outputs will end up being a sequence with a mapping of indexes to words a in the summaries_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' actor details does confirms yesterday dancers politics adam enjoys short action tent efforts reagan reagan wind november development countries porn mental learned development want well palace we victims between lift golden so are increasing tent actions foot between row between compared witness outbreak apparel cable kenny korea faith 30 bigger systems taylor moved customers overcome announced turkey breathe reject bodies spread compared'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test summary done by model with random weights - now need to set up training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "s2s = Seq2Seq(enc, dec)\n",
    "preds = s2s(article_tensor, summary_tensor)\n",
    "\n",
    "final_pred = \"\"\n",
    "for pred_tensor in preds[1:]:\n",
    "    for pred in pred_tensor:\n",
    "        guess_idx = pred.argmax(0)\n",
    "        word_guess = summaries_vocab.lookup_token(guess_idx)\n",
    "        final_pred += ' ' + word_guess\n",
    "\n",
    "final_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "train_data = data[:100]\n",
    "test_data = data[100:110]\n",
    "\n",
    "## add '<sos>' and '<eos>' tokens to each input and target\n",
    "train_data.loc[:, 'article'] = '<sos> ' + train_data['article'] + ' <eos>'\n",
    "train_data.loc[:, 'highlights'] = '<sos> ' + train_data['highlights'] + ' <eos>'\n",
    "test_data.loc[:, 'article'] = '<sos> ' + test_data['article'] + ' <eos>'\n",
    "test_data.loc[:, 'highlights'] = '<sos> ' + test_data['highlights'] + ' <eos>'\n",
    "\n",
    "## reset indexes\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "## tokenize and convert to numerical form\n",
    "train_data['article'] = train_data['article'].apply(lambda text: [articles_vocab[token] for token in tokenizer(text)])\n",
    "train_data['highlights'] = train_data['highlights'].apply(lambda text: [summaries_vocab[token] for token in tokenizer(text)])\n",
    "\n",
    "test_data['article'] = test_data['article'].apply(lambda text: [articles_vocab[token] for token in tokenizer(text)])\n",
    "test_data['highlights'] = test_data['highlights'].apply(lambda text: [summaries_vocab[token] for token in tokenizer(text)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, articles, highlights):\n",
    "        self.articles = articles\n",
    "        self.highlights = highlights\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.articles[idx], self.highlights[idx]\n",
    "    \n",
    "def collate_fn(data):\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    articles, highlights = zip(*data)\n",
    "\n",
    "    # pad articles and highlights\n",
    "    pad_idx = summaries_vocab[\"<pad>\"]\n",
    "    articles = pad_sequence([torch.tensor(sentence) for sentence in articles], padding_value=pad_idx, batch_first=True)\n",
    "    highlights = pad_sequence([torch.tensor(sentence) for sentence in highlights], padding_value=pad_idx, batch_first=True)\n",
    "\n",
    "    return articles, highlights\n",
    "\n",
    "# Dataset creation\n",
    "train_dataset = TextDataset(train_data['article'].tolist(), train_data['highlights'].tolist())\n",
    "test_dataset = TextDataset(test_data['article'].tolist(), test_data['highlights'].tolist())\n",
    "\n",
    "batch_size = 16\n",
    "# Dataloader creation\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Epoch: 0, Loss: 7.297936303274972\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Epoch: 1, Loss: 6.137227603367397\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Epoch: 2, Loss: 5.989615372249058\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "n_epochs = 3\n",
    "encoder_embedding_size = 256\n",
    "decoder_embedding_size = 256\n",
    "hidden_size = 512\n",
    "n_layers = 1\n",
    "dropout_p = 0.2\n",
    "decoder_output_size = decoder_vocab_size\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model and optimizer definition\n",
    "encoder = Encoder(encoder_vocab_size, encoder_embedding_size, hidden_size, n_layers, dropout_p).to(device)\n",
    "decoder = Decoder(decoder_vocab_size, decoder_embedding_size, hidden_size, decoder_output_size, n_layers, dropout_p).to(device)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "pad_idx = summaries_vocab[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (articles, highlights) in enumerate(train_dataloader): # dataloader needs to be defined\n",
    "\n",
    "        articles = articles.to(device)\n",
    "        highlights = highlights.to(device)\n",
    "\n",
    "        # Permute the dimensions of the tensors\n",
    "        articles = articles.permute(1, 0)\n",
    "        highlights = highlights.permute(1, 0)\n",
    "        \n",
    "        # Pass inputs through the model\n",
    "        output = model(articles, highlights)\n",
    "\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        highlights = highlights[1:].reshape(-1)\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute loss and backpropagate\n",
    "        loss = criterion(output, highlights)\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip to avoid exploding gradient issues\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        print(batch_idx)\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Loss: {total_loss / len(train_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the the the . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test summary done by model after one epoch of training in 1000 samples\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "preds = model(article_tensor, summary_tensor)\n",
    "\n",
    "final_pred = \"\"\n",
    "for pred_tensor in preds[1:]:\n",
    "    for pred in pred_tensor:\n",
    "        guess_idx = pred.argmax(0)\n",
    "        word_guess = summaries_vocab.lookup_token(guess_idx)\n",
    "        final_pred += ' ' + word_guess\n",
    "\n",
    "final_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq2seq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
